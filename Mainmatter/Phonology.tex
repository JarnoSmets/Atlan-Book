

\lettrine{A} language is a system through which an individual can communicate with others, which is structured in grammar and vocabulary. Languages are usually spoken, but can also be conveyed by signs as with sign language, or with script. The definition of language is quite a contested topic. Multiple theories about the purpose of language have been proposed. One of the first definitions of languages was put forward by Ferdinand De Saussure. De Saussure saw language as self-contained, self-regulating system, in which the elements are characterised by their relationship with other elements in the system. De Saussure named his vision on linguistics ‘semiology’, but this was posthumously named structuralism by other linguists (Matthews, 2014). 

Nowadays, linguistic scholars deem the structuralist approach outdated, and favour more recent explanations. While some linguistic scholars such as Noam Chomsky and Steven Pinker see language as a biological, formal, or ‘mathematical’ system of signs that are dictated by grammatical rules to convey an utterance (Chomsky, 2002; Pinker, 1994), other scholars such as Nicholas Evans pursue the more ‘functional’ approach and see language as a system of communication that allows for the exchange of utterances (Evans \& Levinson, 2009). One other view sees language primarily and purely as a ‘tool’ that can be used for humans to undertake linguistic behaviour, in that language is solely a means of producing and understanding utterances that evolved over the course of human history (Fitch et al., 2005). 

Note that these definitions more or less convey the same meaning: “a system through which an individual can communicate”. The difference between these views is not so much what language is for, but what it emphasises. They are not mutually exclusive to a certain degree. Nonetheless, contemporary scholars predominantly adopt Chomsky's biological approach. However, even this view has been contested, on the grounds that neuroscientific studies have found neither biological nor neurologic evidence for the existence of Chomsky’s theory on the application of WH-questions, i.e., what, where, when, who(m/se), why, which, and how (Kluender \& Kutas, 1993). 

English is still the most spoken language of academia worldwide, and the \textit{lingua franca} of the western world (Mauranen, 2003). It has not, however, gained this position because it is easy to speak or learn. Pronunciation of English vowels, for example, is unlike its graphemic notation, due to phonological shifts of vowels after the standardisation of English spelling in the 15th and 16th centuries (Denham \& Lobeck, 2007). English did not gain its position because of the purported absence of cultural influence of English, as stated by Knapp and Meierkord (2002). English fulfils the need of a global lingua franca, as it has spread to large areas of the world due to various factors. These include the adoption of the Latin script worldwide, the invention of the internet and its first widespread use in the United States of America. 

The development of the American research university and subsequent adoption of English as \textit{the} academic language have also been of tremendous importance its widespread use. However, there exist more sinister factors as well, such as widespread colonization brought about by the British, American cultural hegemony, and the spread of Christianity through western missionaries (Ariza \& Navarro, 2006). The use of English in academic language has long been postulated by some to be ‘neutral’, i.e., free of cultural influences (House, 2003). 

However, as of late this claim has been challenged. Scholars such as Pölzl and Seidlhofer (2006) and Knapp and Meierkord (2002) have claimed that English is ‘imperialistic’ by definition due to the use of English by colonists. These colonists subsequently decreed that English would be the sole language to be spoken in countries which do not have English as its endogenous language, and as such was seen as a form of oppression (Macedo \& Bartolomé, 2014).

Other scholars have presumed that English can be ‘neutral’ to a certain degree, and that it is up to the speaker of a language to give partiality to one’s words and actions (Norton, 1997). If this view is mirrored against the notion of the impartiality of language and that language and culture are interwoven to their very core as famously articulated by Kramsch (2014), it is possible to surmise that any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation is intrinsically biased, due to the fact that culture and language are inherently linked (Lyons, 1991). 

Atlan is designed to be an auxiliary constructed language, a language that is created with the purpose of facilitating communication between people who have different native languages. This decision has been made because we are of the opinion that a language that is used in academic context should be neutral. This does not imply that the language shall solely be used for academic purposes, nor does it mean that it should replace other languages. 

With the creation of the language, multiple goals have been kept in mind. The primary purpose in the creation of a language is to be as culturally neutral as possible, so that no group of people will be especially favoured or disfavoured when learning the language with regards to the similarity to their own. Creating a language from scratch can procure this cornerstone. 

Another main goal is that the language should both be easy to speak and understand. The notion of unambiguity is another tenet, with the goal of reducing confusion or misinterpretation within communication as much as possible. This means being as sparse as possible, with different elements of the language, where simplicity is key, and complexity should arise from the combination of the basic elements. This is, of course, of utmost importance in phonology and morphology. If a differing consonant is used, it would change the entirety of the word. The same applies to morphology, where the distinction needs to be made between who the actor and who the recipient is. 

This paper will serve as an overview regarding the phonological and morphological considerations that have been made for the language. In the first section of this paper, I will elaborate on the neurology concerning speech and language. The second section will cover the choices that have been made regarding the phonology for consonants, vowels, and prosody. Finally, I will close this paper by summarising what has been stated, and giving some concluding remarks.

\vfill

\section{The Neurologic Basis of Language}

Neurolinguistics is the study of how the brain produces, comprehends, and acquires language. 
It combines both the framework of humanities, namely the language aspect, with a neuroscience approach. The two traditional brain areas that are correlated with the production and comprehension of language and speech with respectively Broca’s area in the frontal lobe, and Wernicke’s area in the temporal lobe (Geschwind, 1972), which are connected through the \textit{fasciculus arcuatus} (Bernal \& Altman, 2010). These areas are not bilaterally localized, and solely exist in the left cerebral hemisphere. 

The production of speech occurs according to three main principles: conceptualization, formulation, and articulation. In the first stage, conceptualization, an individual with the intention to create speech links the desired concept to the particular spoken words. This preverbal message contains the to-be conveyed thoughts to be expressed. The second stage is formulation, in which the linguistic form for the desired message is formulated. Here, knowledge of grammar, phonology, and phonetics is applied to the preverbal message. The third stage is the articulation of the message, in which motor functions are activated to produce the utterance. 

The perception of language or speech begins at the level of the sound signal and the process of audition. Subsequently, speech sounds are further processed in order to gain information regarding acoustic cues and phonetics. This information can then be used for processes that are considered to be ‘higher-level’ language processes, such as word recognition (Levelt, 1999). These produced sounds are then further processed in the auditory cortex of the brain. 

Research has indicated that the auditory cortex processes voiceless and voiced phonemes differently in ferrets, which have similar structures in the processing of auditive information when compared to humans (Mesgarani et al., 2008). Phonemes are, put very simply, sounds, or the smallest units of speech. Phonemes are usually divided into consonants and vowels (Yallop \& Fletcher, 2007). Consonants are created by constricting the airflow in the vocal tract when air is forced out of the lungs, and is mostly done by the tongue. 

Some consonants can also be created by, among others, the nose and vocal tract. Voiced consonants are consonants that incorporate the vibration of the vocal cords when the articulation of the letter occurs. Some examples of voiced consonants are the /b/, /d/, and /g/. Voiceless consonants on the other hand do not make use of the vocal cords. Examples of voiceless include /p/, /t/, and /k/. Some languages, such as Arabic, do not have the voiceless bilabial plosive /p/ in their phonological inventory (Al-Ani, 1970). When a speaker of Arabic wants to say the word ‘pizza’, they would pronounce it as ‘bizza’, for the voiced bilabial plosive /b/ is used instead of the /p/. If an Italian on holiday in an Arabic-speaking country would order a pizza, pronouncing the word with the voiceless bilabial plosive /p/, a monolingual speaker of Arabic would not have any hindrances whatsoever with the comprehension of the utterance (Versteegh, 2014). 

This can be linked to another research by Liégeois-Chauvel et al. (1999) on the inquiry of the perception of voiced and voiceless phonemes. In this research, a speaker produced voiceless and voiced phonemes, with the following vowel being /a/ (/pa/, /ta/, /ka/ for voiceless, and /ba/, /da/, /ga/ for voiced) in a random order. Neurologic tests were carried out usinga tool called ‘electroencephalography’ (EEG). An EEG maps where in the brain electrical pulses occur, i.e., where and which areas of the brain are activated when an individual is exposed to stimuli. The EEG has shown that the auditory cortex is able to process syllables with voiced consonants from syllables with voiceless consonants in the left hemisphere, however, the right hemisphere was not able to make this distinction and solely processed acoustic stimuli. Furthermore, the auditory cortex was not able to differentiate syllables with voiced consonants and voiceless consonants. The results from the EEG showed no discernible differences between syllables with voiced and voiceless consonants. However, a differential coding of voiced and voiceless syllables is preserved. This would still mean that an individual is able to distinguish these phonemes (Liégeois-Chauvel et al., 1999).


\section{Consonants in Atlan}
As previously stated, the word ‘pizza’ would be pronounced as ‘bizza’ by according to Arabic phonology (Al-Ani, 1970). The example also states that ‘pizza’ and ‘bizza’ would both be understood as the same word. This is because in Arabic, the ‘b’ and ‘p’ are variants of the same phoneme. This is called allophony. 

Furthermore, certain languages (or language families) use scripts that do not implicate the voicing of a consonant, such as Tamil. Tamil uses both voiced and unvoiced consonants, however, it is decided by context (e.g., a -linguistic- register), and not by its script. A consonant being voiced or unvoiced does not imply that a word gets a whole new meaning, but gives meaning to the context of the word. Consonant voicing thus is not contrastive in Tamil (Keane, 2004; Schiffman \& Arokianathan,1986). Regarding the phonology of our language, the decision has been made that both voiceless and voiced consonants are allophones. For example, a speaker of our language would perceive both the voiced bilabial plosive /b/ and voiceless bilabial plosive /p/ as the same phoneme.  

The script is meant to reflect this, as is the case with Tamil. Furthermore, because not every language has the same set of phonemes nor the same number of phonemes, we have decided that nine distinctive categories should be made. The phonemes that belong to each respective category are allophones in our language. The categories in this were chosen according to mutual intelligibility, proximity according to the consonantal chart of the International Phonetic Alphabet, and manner of articulation(Ladefoged, 1999). Furthermore, consideration has been given to the frequency of each phoneme and its subsequent category.Every category contains a phoneme that has a high rate of frequency in languages worldwide. In order to retrieve theinformation regarding salience of the phonemes, the UCLA Phonological Segment Inventory Database (UPSID) and the Phonetics Information Base and Lexicon (PHOIBLE) were used(Maddieson, 1984, 1986; Moran \& McCloy, 2019). These databases document the frequency of every existing phoneme. 

The categories are as follows: The first category contains the (bi-)labial plosives [b, p]. The bilabial plosives are found in 98.89\% of all languages worldwide according to UPSID. The second category consists of the coronal plosives, i.e., the dental, dento-alveolar, alveolar, and retroflex plosives, [t, d, \textrtailt, \textrtaild]. The coronal plosives are found in almost every language according to Liberman \textit{et al.} (1967), however, no exact percentage is given regarding its frequency. The third category contains the dorsal plosives and dorsal fricatives [k, g, q, \textscg] and [x, \textgamma, \textchi, \textinvscr]. The dorsal plosives and fricatives are found in 99.30\% of all languages worldwide according to PHOIBLE and UPSID. The fourth category consists solely of the bilabial and labiodental nasal [m, \textltailm]. According to UPSID, PHOIBLE, and Maddieson(2013a), the bilabial nasal is the phoneme with the highest degree of frequency worldwide, with over 96\% of all languages containing it. The fifth category consists of the coronal and dorsal nasals [n, \textrtailn ,  \textltailn, \ng, \textscn]. No exact percentage is known of the frequency of the non-bilabial nasals, however PHOIBLE states that over 80\% of all languages contain a phoneme of this category. 

The sixth category is what is called the ‘liquid’ consonants. All trills, laterals, and lateral approximants, as well as the coronal and dorsal flaps and taps [r, \textscr , \textfishhookr, \textrtailr, \textbeltl, \textlyoghlig, \textturnr, \textturnrrtail, l, \textrtaill, $\lambda$, \textscl, \textturnmrleg,  \textturnlonglegr] belong to this category. This class is considerable in size, but carefully chosen. Many languages contain one of these consonants, and differing phonemes are usually considered allophones if a differing phoneme is used, as is the case with /r/ and /l/ in Japanese and Korean (Ladefoged \& Maddieson, 1996;Maddieson, 2013b; Takgi \& Mann, 1995). No exact percentage is given for the frequency of these liquids. 

To the seventh category belong the labial fricatives and labial approximants [ⱱ, \textphi, \textbeta, f, v, \texttheta, \dh, \textscriptv , \textturnw, w]. These phonemes are found in 84.49\% of languages worldwide according to PHOIBLE and UPSID. The eight category are the coronal sibilant fricatives [s, z, \textesh, \textyogh, \textrtails , \textrtailz, \textctc, \textctz,  \texththeng]. According to UPSID, these phonemes are found in 88.03\% of languages worldwide. The ninth category consists of the palatal consonants [\c{c}, \textctj, j], which according to UPSID and PHOIBLE are found in 90\% of all languages. A tenth quasi-category was made for glottal and pharyngeal consonants; however, we have decided to give these phonemes no meaning. 

Atlan also employs a glottal stop [\textglotstop], however this sound is not notated in its orthography. Rather, it functions to differentiate two of the same vowels when placed next to one another. For example, ‘KA.AK’ could be confused with ‘KAK’ if there is no pronounced distinction between the two syllables, therefore the former should be pronounced as ‘KA\textglotstop AK’. 


\section{Vowels in Atlan}

Categorising the vowels was considerably more difficult, considering that vowels cannot be placed on an axis of ‘place of articulation’ and ‘manner of articulation, as is the case with consonants (Ladefoged, 1999). Vowels can be placed on a spectrum, with one axis from ‘close’ to ‘open’, and another from ‘front’ to ‘back’. The close-to-open axis refers the position of the tongue placed against the roof of the mouth. ‘Close’ in this context means that the tongue is positioned as close as possible to the roof of the mouth as it can be without creating a constriction, whereas ‘open’ means that the tongue is positioned as far as possible from the roof of the mouth. The front-to-back axis refers to the position of the tongue in the mouth. ‘Front’ in this context means that the tongue is positioned as far forward as possible in the mouth, ‘back’ means that the tongue is positions as far backwards as possible in the mouth (Yallop \& Fletcher, 2007). 

Vowels considered to be close-front include [i] as in the English word ‘fr{\bf ee}’ and the Dutch ‘v{\bf ie}ren’, and close-back include [u] as in the Dutch ‘v{\bf oe}t’ and the English ‘b{\bf oo}t’. Open-front vowelsinclude [a] as in the British English ‘hat’, and open-back include [ɑ] as in the Dutch ‘bad’ (Gussenhoven, 1992; Roach, 2004). 

Because the quality of vowels is a spectrum and not every vowel exists in every language, a certain degree of allophony exists in vowels as with consonants. In Indonesian, [ɪ] and [ʊ] are allophones of /i/ and /u/, while in Dutch they are contrastive (Gussenhoven, 1992; Soderberg \& Olson, 2008). 

For our language, five categories of vowels were made. As with the consonants, these are based on the salience of the vowels and its frequency in languages worldwide. The data regarding this is based on the same tools as for consonants; UPSID and PHOIBLE. Three of these categories were easily made because most languages contain this respective vowel. These are, from high to low frequency, [i, u, a], with respectively 92, 88, and 86\% occurrence in languages worldwide. For the two remaining categories, a substantial lower frequency is noted for [e, o], with respectively 61 and 60\% of the languages worldwide containing the vowel according to UPSID and \\ PHOIBLE. 

These five categories were chosen because these five vowels are found in every language, and the frequency of the vowels [e, o] were found in roughly the same percentages in language families worldwide, with the exception of (some) Australian languages (Butcher, 2018; Moran \& McCloy, 2019). 

Another extra vowel is used in our constructed language, namely the schwa [ə], but this vowel is not notated. Its function is to differentiate two of the same consonants that occur next to one another, similar to the use of the glottal stop. For example, in spoken Atlan, the difference between ‘AK.KA’ would be barely distinguishable from ‘A.KA’, therefore the former would be pronounced ‘AKəKA’ to retain the distinction. 

\section{Decisions regarding the phonetics: tone and prosody}

\noindent In some languages, tone (i.e., the use of pitch), is a meaning distinguishing feature. For example, Mandarin is a tonal languageand depending on the pitch or variation in pitch, the word ‘ma’ can have five different meanings, such as horse, mother, scold, or as a marker for a question (Lee et al., 1996). Pitch can be as important as vowels themselves for comprehension of words and grammatical functions. Following on from the previous section, we decided that phonetic properties, such as tone, have no semantic nor pragmatic value. Prosody has no intrinsic value either. Prosody consists of intonation and rhythm. Intonation are the changes in pitch used for, e.g., conveying the speaker's attitudes and emotions, or to highlight or focus an expression. The concept of rhythm in language is dubious at best, and is perhaps better explained by the notion that the perception of rhythm is based on the language that an individual already speaks, and is thus irrelevant to precisely define (Arvaniti, 2012). Likewise, lexico-semantic characteristics are solely conveyed through phonemes, not the intonation or pitch of said phonemes. This is due to the fact that only a select few languages make use of pitch differences for semantic meanings, such as Mandarin, Cantonese, Vietnamese, Yoruba, and Navajo (Bauer \& Benedict, 1997; Yip, 2002). 
  
\section{Morphology in Atlan} 
  
\noindent What is the difference between ‘walking’ and ‘walked’? Both words convey the meaning that something or someone is moving at a regular pace by lifting and setting down each foot in turn, never having both feet off the ground at once. However, while they convey the same meaning in movement, saying: “Stijn walking there yesterday,” would be incorrect, as is “Jep is walked there”. The difference here is the suffix: a morpheme added at the end of a word to form a derivative (e.g. -ation, -fy, -ing). A morpheme, and its subject of study called morphology, examine the smallest meaningful units of language, which can be individual words or parts of words (Matthews, 1991). The main goal of morphology is to understand how words are constructed and how they convey meaning. It analyzes the various types of morphological processes, such as affixation, where morphemes are added in, around, before, or after a word, or compounding, where two or more words are combined to form a new one, and inflection, in which the form of a word is altered to indicate grammatical information like tense, number, or gender (Booij, 2007). 

Some languages make heavy use of these morphemes for these context-related factors, other languages do not. Languages exist on a continuum in regard to morphology, but can be more or less categorised. On one end of the spectrum exist isolating of analytic languages, in which words are composed primarily of individual morphemes that are each distinct and carry a specific meaning, such as Vietnamese (Comrie, 1989). In isolating languages, each morpheme generally corresponds to a specific concept or grammatical function. 

On the other end of the spectrum, there are polysynthetic languages, in which words are composed of multiple morphemes that are fused together to express complex ideas and convey a wealth of information within a single word. In polysynthetic languages, a single word can contain a combination of roots, affixes, and grammatical markers, allowing for the expression of entire sentences' worth of information (Baker, 1998). In polysynthetic languages, the process of word formation involves extensive morphological affixation, compounding, and incorporation. An example of a polysynthetic language is Nahuatl, spoken in Mesoamerica before the colonization by Spanish conquistadores (Rolstad, 2001; Suarez, 1983). Somewhere in this continuum, agglutinative languages exist. 

In agglutinative languages, morphemes are typically added to the root or stem of a word to express various grammatical features such as tense, aspect, mood, number, case, and person. Unlike other synthetic languages like Nahuatl, agglutinative languages maintain a one-to-one correspondence between morphemes and specific grammatical functions. Generally, agglutinative languages have a great degree of transparency in their morphological systems. The term “transparent” means that the relationship between the morphemes and their meanings is relatively straightforward and predictable. The affixes are typically added in a consistent and regular manner, allowing for clear distinctions between different grammatical features (Durrant, 2013). A wonderful example of an agglutinative language is Turkish (Lewis, 2001). 

Atlan, like Turkish, makes use of an agglutinative system for morphemes. One of the core elements of Atlan is that unambiguity is a prerequisite. In choosing an agglutinative system for morphemes, we are of the opinion that this keystone has been achieved.  
  
  
\section{Concluding remarks} 
  
\noindent In this paper, we explored the neurologic basis of language and discussed the phonological considerations for the creation of an international auxiliary constructed language. We highlighted the interconnectedness between language and the brain, and the subsequent choices regarding the phonetic, phonological, and morphological system.

Language is a complex system that is not merely a tool for communication but a reflection of our culture and identity. While constructing a language that is completely devoid of bias may be challenging, striving for neutrality and inclusivity is a worthy endeavour. The creation of a neutral and accessible language has the potential to enhance global communication, foster cultural exchange, and promote inclusivity. While language will always carry cultural influences, our efforts to create a more neutral language reflect our commitment to open dialogue and mutual understanding in an increasingly interconnected world. 

\vfill
